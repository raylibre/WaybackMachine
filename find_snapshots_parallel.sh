#!/bin/bash

DOMAIN="$1"
TARGET_DATE="$2"
MAX_PARALLEL="${3:-8}"  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤

if [ -z "$DOMAIN" ] || [ -z "$TARGET_DATE" ]; then
    echo "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: $0 DOMAIN TARGET_DATE [MAX_PARALLEL]"
    echo "–ü—Ä–∏–º–µ—Ä: $0 ba.org.ua 20191115 8"
    echo "TARGET_DATE –≤ —Ñ–æ—Ä–º–∞—Ç–µ YYYYMMDD"
    echo "MAX_PARALLEL - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 8)"
    exit 1
fi

# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç—ã
if ! [[ "$TARGET_DATE" =~ ^[0-9]{8}$ ]]; then
    echo "‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç—ã. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ YYYYMMDD"
    exit 1
fi

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –º–∞—Å—Ç–µ—Ä-—Å–ø–∏—Å–∫–∞
MASTER_LIST="${DOMAIN}_master_list.json"
if [ ! -f "$MASTER_LIST" ]; then
    echo "‚ùå –ú–∞—Å—Ç–µ—Ä-—Å–ø–∏—Å–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω: $MASTER_LIST"
    echo "–°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ: ./create_master_list.sh $DOMAIN"
    exit 1
fi

echo "‚ö° –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–´–ô –ø–æ–∏—Å–∫ —Å–Ω–∞–ø—à–æ—Ç–æ–≤ –¥–ª—è $DOMAIN –Ω–∞ –¥–∞—Ç—É $TARGET_DATE"
echo "=============================================================="
echo "üî• –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤: $MAX_PARALLEL"

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –¥–∞—Ç—ã (–∫—Ä–æ—Å—Å–ø–ª–∞—Ç—Ñ–æ—Ä–º–µ–Ω–Ω–∞—è)
calculate_date() {
    local base_date="$1"
    local days_offset="$2"

    local year="${base_date:0:4}"
    local month="${base_date:4:2}"
    local day="${base_date:6:2}"
    local formatted_date="$year-$month-$day"

    if [[ "$OSTYPE" == "darwin"* ]]; then
        if command -v gdate >/dev/null 2>&1; then
            gdate -d "$formatted_date $days_offset days" +%Y%m%d
        else
            python3 -c "
import datetime
base = datetime.datetime.strptime('$formatted_date', '%Y-%m-%d')
result = base + datetime.timedelta(days=$days_offset)
print(result.strftime('%Y%m%d'))
"
        fi
    else
        date -d "$formatted_date $days_offset days" +%Y%m%d
    fi
}

# –í—ã—á–∏—Å–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–µ –æ–∫–Ω–æ
FROM_DATE=$(calculate_date "$TARGET_DATE" "-90")
TO_DATE=$(calculate_date "$TARGET_DATE" "90")

echo "üìÖ –í—Ä–µ–º–µ–Ω–Ω–æ–µ –æ–∫–Ω–æ: $FROM_DATE - $TO_DATE"

# –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
TEMP_DIR="temp_parallel_$$"
mkdir -p "$TEMP_DIR"

echo "üß† –†–∞–∑–±–∏–≤–∞–µ–º –º–∞—Å—Ç–µ—Ä-—Å–ø–∏—Å–æ–∫ –Ω–∞ batch'–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏..."

# –†–∞–∑–±–∏–≤–∞–µ–º –º–∞—Å—Ç–µ—Ä-—Å–ø–∏—Å–æ–∫ –Ω–∞ —á–∞—Å—Ç–∏
total_urls=$(jq length "$MASTER_LIST")
urls_per_batch=$(( (total_urls + MAX_PARALLEL - 1) / MAX_PARALLEL ))

echo "üìä –í—Å–µ–≥–æ URL: $total_urls"
echo "üì¶ URL –≤ –±–∞—Ç—á–µ: $urls_per_batch"

# –°–æ–∑–¥–∞–µ–º batch —Ñ–∞–π–ª—ã
for i in $(seq 0 $((MAX_PARALLEL - 1))); do
    start_index=$((i * urls_per_batch))
    jq ".[$start_index:$((start_index + urls_per_batch))]" "$MASTER_LIST" > "$TEMP_DIR/batch_$i.json"
done

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ batch'–∞
process_batch() {
    local batch_id="$1"
    local batch_file="$TEMP_DIR/batch_$batch_id.json"
    local result_file="$TEMP_DIR/result_$batch_id.json"

    if [ ! -s "$batch_file" ]; then
        echo "[]" > "$result_file"
        return
    fi

    echo "üöÄ Batch $batch_id: —Å—Ç–∞—Ä—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏"

    # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö URL –∏–∑ batch'–∞ –¥–ª—è –æ–¥–Ω–æ–≥–æ CDX –∑–∞–ø—Ä–æ—Å–∞
    local urls_list
    urls_list=$(jq -r '.[].original' "$batch_file" | head -20 | tr '\n' ' ')  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 20 URL –Ω–∞ –∑–∞–ø—Ä–æ—Å

    if [ -z "$urls_list" ]; then
        echo "[]" > "$result_file"
        return
    fi

    # –§–æ—Ä–º–∏—Ä—É–µ–º URL –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
    local url_pattern
    if [ $(jq length "$batch_file") -eq 1 ]; then
        # –û–¥–∏–Ω URL
        url_pattern=$(jq -r '.[0].original' "$batch_file")
    else
        # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ URL - –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–æ–º–µ–Ω + wildcards
        url_pattern="${DOMAIN}/*"
    fi

    # CDX –∑–∞–ø—Ä–æ—Å –¥–ª—è batch'–∞
    local cdx_response
    cdx_response=$(curl -s "http://web.archive.org/cdx/search/cdx?url=${url_pattern}&from=$FROM_DATE&to=$TO_DATE&output=json&fl=timestamp,original,statuscode,mimetype,length&filter=statuscode:200&filter=mimetype:text/html&limit=10000" 2>/dev/null)

    if [ -z "$cdx_response" ] || [ "$cdx_response" = "[]" ]; then
        echo "[]" > "$result_file"
        echo "‚ö†Ô∏è  Batch $batch_id: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö"
        return
    fi

    # Python –æ–±—Ä–∞–±–æ—Ç–∫–∞ batch'–∞
    python3 << EOF
import json
import sys

try:
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ batch'–∞
    with open("$batch_file", "r") as f:
        batch_urls = json.load(f)

    if not batch_urls:
        with open("$result_file", "w") as f:
            json.dump([], f)
        sys.exit(0)

    # –°–æ–∑–¥–∞–µ–º set URL –∏–∑ batch'–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
    batch_url_set = {item['original'] for item in batch_urls}

    # –ü–∞—Ä—Å–∏–º CDX –æ—Ç–≤–µ—Ç
    cdx_data = json.loads('$cdx_response')

    if len(cdx_data) <= 1:
        with open("$result_file", "w") as f:
            json.dump([], f)
        sys.exit(0)

    # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –ø–æ URL
    target_timestamp = int("${TARGET_DATE}000000")
    url_best_snapshot = {}

    for row in cdx_data[1:]:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
        if len(row) >= 5:
            try:
                timestamp, original, statuscode, mimetype, size = row[:5]

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ URL –∏–∑ –Ω–∞—à–µ–≥–æ batch'–∞
                if original not in batch_url_set:
                    continue

                snap_timestamp = int(timestamp)
                time_diff = abs(snap_timestamp - target_timestamp)

                if original not in url_best_snapshot or time_diff < url_best_snapshot[original]['time_diff']:
                    url_best_snapshot[original] = {
                        'timestamp': timestamp,
                        'original': original,
                        'statuscode': statuscode,
                        'size': int(size) if size.isdigit() else 0,
                        'time_diff': time_diff,
                        'days_diff': time_diff // 1000000
                    }
            except (ValueError, IndexError):
                continue

    # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    results = []
    for url_data in url_best_snapshot.values():
        snapshot = {
            'archive_url': f"https://web.archive.org/web/{url_data['timestamp']}/{url_data['original']}",
            'timestamp': url_data['timestamp'],
            'original_url': url_data['original'],
            'statuscode': url_data['statuscode'],
            'size': url_data['size'],
            'days_diff': url_data['days_diff']
        }
        results.append(snapshot)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    with open("$result_file", "w") as f:
        json.dump(results, f, indent=2)

    print(f"‚úÖ Batch $batch_id: –Ω–∞–π–¥–µ–Ω–æ {len(results)} —Å–Ω–∞–ø—à–æ—Ç–æ–≤")

except Exception as e:
    print(f"‚ùå Batch $batch_id: –æ—à–∏–±–∫–∞ - {e}")
    with open("$result_file", "w") as f:
        json.dump([], f)
EOF

    echo "‚úÖ Batch $batch_id: –∑–∞–≤–µ—Ä—à–µ–Ω"
}

# –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ xargs
export -f process_batch
export TEMP_DIR FROM_DATE TO_DATE TARGET_DATE DOMAIN

echo ""
echo "üî• –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É ($MAX_PARALLEL –ø—Ä–æ—Ü–µ—Å—Å–æ–≤)..."

# –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
seq 0 $((MAX_PARALLEL - 1)) | xargs -n 1 -P "$MAX_PARALLEL" -I {} bash -c 'process_batch {}'

echo ""
echo "üîó –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã..."

# –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
SNAPSHOT_FILE="${DOMAIN}_snapshots_${TARGET_DATE}.json"

python3 << EOF
import json
import glob

all_snapshots = []

# –ß–∏—Ç–∞–µ–º –≤—Å–µ result —Ñ–∞–π–ª—ã
for result_file in glob.glob("$TEMP_DIR/result_*.json"):
    try:
        with open(result_file, "r") as f:
            batch_results = json.load(f)
            all_snapshots.extend(batch_results)
    except:
        continue

# –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ original_url
seen_urls = set()
unique_snapshots = []
for snapshot in all_snapshots:
    url = snapshot['original_url']
    if url not in seen_urls:
        seen_urls.add(url)
        unique_snapshots.append(snapshot)

# –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –±–ª–∏–∑–æ—Å—Ç–∏ –∫ –¥–∞—Ç–µ
unique_snapshots.sort(key=lambda x: x['days_diff'])

# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
with open("$SNAPSHOT_FILE", "w") as f:
    json.dump(unique_snapshots, f, indent=2, ensure_ascii=False)

print(f"üìÑ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–Ω–∞–ø—à–æ—Ç–æ–≤: {len(unique_snapshots)}")
EOF

# –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
rm -rf "$TEMP_DIR"

# –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
if [ -f "$SNAPSHOT_FILE" ]; then
    final_count=$(jq length "$SNAPSHOT_FILE" 2>/dev/null || echo "0")

    echo ""
    echo "‚ö° –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–´–ô –ü–û–ò–°–ö –ó–ê–í–ï–†–®–ï–ù!"
    echo "==============================="
    echo "üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:"
    echo "  üìÑ URL –≤ –º–∞—Å—Ç–µ—Ä-—Å–ø–∏—Å–∫–µ: $total_urls"
    echo "  üì∏ –ù–∞–π–¥–µ–Ω–æ —Å–Ω–∞–ø—à–æ—Ç–æ–≤: $final_count"
    echo "  ‚úÖ –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: $(( final_count * 100 / total_urls ))%"

    if [ "$final_count" -gt 0 ]; then
        echo ""
        echo "üéØ –¢–û–ü-5 –±–ª–∏–∂–∞–π—à–∏—Ö —Å–Ω–∞–ø—à–æ—Ç–æ–≤:"
        jq -r '.[:5] | .[] | "  \(.days_diff) –¥–Ω–µ–π: \(.original_url)"' "$SNAPSHOT_FILE"
    fi

    echo ""
    echo "üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç: $SNAPSHOT_FILE"
    echo "üöÄ –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥: poetry run wayback-analyzer download-snapshot $DOMAIN --date $TARGET_DATE"
else
    echo "‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ñ–∞–π–ª–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
    exit 1
fi