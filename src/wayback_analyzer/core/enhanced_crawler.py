"""–£–ª—É—á—à–µ–Ω–Ω—ã–π –∫—Ä–∞—É–ª–µ—Ä –¥–ª—è –º–∞—Å—Å–æ–≤–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞."""

import requests
import logging
import json
import time
from bs4 import BeautifulSoup
from typing import Set, List, Dict, Optional, Any
from urllib.parse import urljoin, urlparse
from pathlib import Path
from datetime import datetime, timedelta

from .storage_manager import StorageManager
from ..utils.rate_limiter import RateLimiter
from ..utils.url_helper import ArchiveUrlHelper


class EnhancedSnapshotCrawler:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –∫—Ä–∞—É–ª–µ—Ä –¥–ª—è –±–æ–ª—å—à–∏—Ö –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Å–∞–π—Ç–æ–≤."""

    def __init__(
            self,
            storage_manager: StorageManager,
            rate_limiter: RateLimiter,
            max_depth: int = 4,
            max_pages: int = 500,
            resume_mode: bool = True
    ):
        self.storage_manager = storage_manager
        self.rate_limiter = rate_limiter
        self.max_depth = max_depth
        self.max_pages = max_pages
        self.resume_mode = resume_mode

        self.visited_urls: Set[str] = set()
        self.found_pages: List[Dict] = []
        self.failed_urls: List[Dict] = []
        self.skipped_urls: List[str] = []

        self.logger = logging.getLogger(__name__)

        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'start_time': None,
            'pages_processed': 0,
            'pages_skipped': 0,
            'pages_failed': 0,
            'total_size_mb': 0,
            'avg_page_size': 0
        }

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ requests session
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (compatible; UkrainePoliticalAnalyzer/1.0; +https://github.com/your/repo)'
        })

        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã –¥–ª—è –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Å–∞–π—Ç–æ–≤
        self.priority_paths = [
            '/about', '/–ø—Ä–æ-–ø–∞—Ä—Ç—ñ—é', '/–ø—Ä–æ–≥—Ä–∞–º–∞', '/program',
            '/news', '/–Ω–æ–≤–∏–Ω–∏', '/blog', '/–±–ª–æ–≥',
            '/deputies', '/–¥–µ–ø—É—Ç–∞—Ç–∏', '/candidates', '/–∫–∞–Ω–¥–∏–¥–∞—Ç–∏',
            '/contacts', '/–∫–æ–Ω—Ç–∞–∫—Ç–∏', '/join', '/–ø—Ä–∏—î–¥–Ω–∞—Ç–∏—Å—è'
        ]

        # –ò—Å–∫–ª—é—á–∞–µ–º—ã–µ –ø—É—Ç–∏
        self.exclude_paths = [
            '/wp-admin', '/admin', '/login',
            '/search', '/–ø–æ—à—É–∫', '/404',
            '.pdf', '.doc', '.zip', '.jpg', '.png', '.gif'
        ]

    def crawl_political_site(
            self,
            site_url: str,
            target_date: str = None,
            callback=None
    ) -> Dict[str, Any]:
        """
        –ü–æ–ª–Ω—ã–π –æ–±—Ö–æ–¥ –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–∞–π—Ç–∞ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π.

        Args:
            site_url: URL —Å–∞–π—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, https://sluga-narodu.com)
            target_date: –¶–µ–ª–µ–≤–∞—è –¥–∞—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ YYYY-MM-DD (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            callback: –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        """

        self.logger.info(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ö–æ–¥ –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–∞–π—Ç–∞: {site_url}")
        self.stats['start_time'] = time.time()

        # –ù–∞—Ö–æ–¥–∏–º –ø–æ–¥—Ö–æ–¥—è—â–∏–π —Å–Ω–∞–ø—à–æ—Ç
        archive_url = self._find_best_snapshot(site_url, target_date)
        if not archive_url:
            raise ValueError(f"–ù–µ –Ω–∞–π–¥–µ–Ω –∞—Ä—Ö–∏–≤–Ω—ã–π —Å–Ω–∞–ø—à–æ—Ç –¥–ª—è {site_url}")

        timestamp, original_url = ArchiveUrlHelper.extract_timestamp_and_original(archive_url)
        target_domain = ArchiveUrlHelper.get_domain(original_url)

        self.logger.info(f"üìÖ –ù–∞–π–¥–µ–Ω —Å–Ω–∞–ø—à–æ—Ç: {timestamp}")
        self.logger.info(f"üåê –¶–µ–ª–µ–≤–æ–π –¥–æ–º–µ–Ω: {target_domain}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º resume —Ä–µ–∂–∏–º
        if self.resume_mode:
            self._load_previous_state(archive_url)

        # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –µ—Å–ª–∏ –Ω–µ resume
        if not self.resume_mode:
            self.visited_urls.clear()
            self.found_pages.clear()
            self.failed_urls.clear()

        # –ù–∞—á–∏–Ω–∞–µ–º —Ä–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –æ–±—Ö–æ–¥
        self._crawl_with_priority(archive_url, target_domain, callback)

        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        end_time = time.time()
        duration = end_time - self.stats['start_time']

        summary = {
            'start_url': archive_url,
            'original_url': site_url,
            'timestamp': timestamp,
            'target_domain': target_domain,
            'total_pages_found': len(self.found_pages),
            'total_pages_failed': len(self.failed_urls),
            'total_pages_skipped': len(self.skipped_urls),
            'crawl_duration_seconds': round(duration, 2),
            'crawl_duration_minutes': round(duration / 60, 2),
            'max_depth_reached': max((page.get('depth', 0) for page in self.found_pages), default=0),
            'avg_page_size_kb': round(self.stats['avg_page_size'] / 1024, 2) if self.stats['avg_page_size'] else 0,
            'total_size_mb': round(self.stats['total_size_mb'], 2),
            'pages_per_minute': round(len(self.found_pages) / (duration / 60), 2) if duration > 0 else 0,
            'pages': self.found_pages,
            'failed_urls': self.failed_urls,
            'skipped_urls': self.skipped_urls[:50]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è JSON
        }

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–≤–æ–¥–∫—É –∏ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        self.storage_manager.save_snapshot_summary(archive_url, summary)
        self._save_crawler_state(archive_url, summary)

        self.logger.info(f"‚úÖ –û–±—Ö–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω:")
        self.logger.info(f"   üìÑ –°—Ç—Ä–∞–Ω–∏—Ü: {len(self.found_pages)}")
        self.logger.info(f"   ‚ùå –û—à–∏–±–æ–∫: {len(self.failed_urls)}")
        self.logger.info(f"   ‚è±Ô∏è  –í—Ä–µ–º—è: {summary['crawl_duration_minutes']:.1f} –º–∏–Ω")
        self.logger.info(f"   üíæ –†–∞–∑–º–µ—Ä: {summary['total_size_mb']:.1f} MB")

        return summary

    def _find_best_snapshot(self, site_url: str, target_date: str = None) -> Optional[str]:
        """–ù–∞–π—Ç–∏ –ª—É—á—à–∏–π –¥–æ—Å—Ç—É–ø–Ω—ã–π —Å–Ω–∞–ø—à–æ—Ç –¥–ª—è —Å–∞–π—Ç–∞."""
        from waybackpy import WaybackMachineCDXServerAPI

        try:
            cdx_api = WaybackMachineCDXServerAPI(site_url, self.session.headers['User-Agent'])

            if target_date:
                # –ò—â–µ–º —Å–Ω–∞–ø—à–æ—Ç—ã —Ä—è–¥–æ–º —Å —Ü–µ–ª–µ–≤–æ–π –¥–∞—Ç–æ–π
                target_dt = datetime.strptime(target_date, '%Y-%m-%d')

                # –ò—â–µ–º –≤ –æ–∫–Ω–µ ¬±30 –¥–Ω–µ–π
                best_snapshot = None
                min_diff = float('inf')

                for snapshot in cdx_api.snapshots():
                    try:
                        snapshot_dt = datetime.strptime(snapshot.timestamp, "%Y%m%d%H%M%S")
                        diff = abs((snapshot_dt - target_dt).days)

                        if diff < min_diff and snapshot.statuscode == '200':
                            min_diff = diff
                            best_snapshot = snapshot

                        # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ –¥–Ω—é, –±–µ—Ä–µ–º –µ–≥–æ
                        if diff == 0:
                            break

                    except (ValueError, AttributeError):
                        continue

                if best_snapshot:
                    self.logger.info(f"üìÖ –ù–∞–π–¥–µ–Ω —Å–Ω–∞–ø—à–æ—Ç –Ω–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–∏ {min_diff} –¥–Ω–µ–π –æ—Ç —Ü–µ–ª–µ–≤–æ–π –¥–∞—Ç—ã")
                    return best_snapshot.archive_url

            else:
                # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–æ—Å—Ç—É–ø–Ω—ã–π —Å–Ω–∞–ø—à–æ—Ç
                for snapshot in cdx_api.snapshots():
                    if snapshot.statuscode == '200':
                        return snapshot.archive_url

            return None

        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Å–Ω–∞–ø—à–æ—Ç–∞: {e}")
            return None

    def _crawl_with_priority(self, start_url: str, target_domain: str, callback=None):
        """–û–±—Ö–æ–¥ —Å –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–µ–π –≤–∞–∂–Ω—ã—Ö —Ä–∞–∑–¥–µ–ª–æ–≤."""

        # –°–Ω–∞—á–∞–ª–∞ –æ–±—Ö–æ–¥–∏–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã
        priority_urls = []
        timestamp, base_url = ArchiveUrlHelper.extract_timestamp_and_original(start_url)

        for priority_path in self.priority_paths:
            priority_url = ArchiveUrlHelper.build_archive_url(timestamp, base_url + priority_path)
            priority_urls.append(priority_url)

        # –î–æ–±–∞–≤–ª—è–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤ –Ω–∞—á–∞–ª–æ
        all_urls = [start_url] + priority_urls

        # –û–±—Ö–æ–¥–∏–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ URL
        for url in all_urls:
            if len(self.found_pages) >= self.max_pages:
                break

            if url not in self.visited_urls:
                self._crawl_recursive(url, target_domain, depth=0, callback=callback)

        # –ï—Å–ª–∏ –µ—â–µ –µ—Å—Ç—å –º–µ—Å—Ç–æ, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—ã—á–Ω—ã–π –æ–±—Ö–æ–¥
        if len(self.found_pages) < self.max_pages:
            self._crawl_recursive(start_url, target_domain, depth=0, callback=callback)

    def _crawl_recursive(self, archive_url: str, target_domain: str, depth: int, callback=None):
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –æ–±—Ö–æ–¥ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫."""

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
        if (depth > self.max_depth or
                len(self.found_pages) >= self.max_pages or
                archive_url in self.visited_urls):
            return

        self.visited_urls.add(archive_url)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–∫–ª—é—á–∞–µ–º—ã–µ –ø—É—Ç–∏
        if any(exclude in archive_url.lower() for exclude in self.exclude_paths):
            self.skipped_urls.append(archive_url)
            self.logger.debug(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω (–∏—Å–∫–ª—é—á–µ–Ω): {archive_url}")
            return

        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
        if callback:
            callback(len(self.found_pages), self.max_pages)

        self.logger.debug(f"üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º (–≥–ª—É–±–∏–Ω–∞ {depth}): {archive_url}")

        try:
            # –°–æ–±–ª—é–¥–∞–µ–º rate limiting
            self.rate_limiter.wait_if_needed()

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª–∏ —É–∂–µ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ (resume)
            if self.resume_mode and self.storage_manager.page_exists(archive_url):
                self.logger.debug(f"‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {archive_url}")
                self.stats['pages_skipped'] += 1
                return

            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
            response = self.session.get(archive_url, timeout=30)

            if response.status_code == 404:
                self.logger.debug(f"üö´ –°—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –∞—Ä—Ö–∏–≤–µ: {archive_url}")
                self.failed_urls.append({
                    'url': archive_url,
                    'error': 'Not found in archive',
                    'status_code': 404,
                    'depth': depth
                })
                return

            if response.status_code != 200:
                self.logger.warning(f"‚ö†Ô∏è HTTP {response.status_code} –¥–ª—è {archive_url}")
                self.failed_urls.append({
                    'url': archive_url,
                    'error': f'HTTP {response.status_code}',
                    'status_code': response.status_code,
                    'depth': depth
                })
                return

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —É—Å–ø–µ—à–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            self._process_successful_page(archive_url, response, target_domain, depth)

            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –æ–±—Ö–æ–¥–∞
            if depth < self.max_depth and len(self.found_pages) < self.max_pages:
                internal_links = self._extract_internal_links_optimized(
                    response.text, archive_url, target_domain
                )

                # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –æ–±—Ö–æ–¥–∏–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏
                for link_url in internal_links:
                    if len(self.found_pages) >= self.max_pages:
                        break
                    self._crawl_recursive(link_url, target_domain, depth + 1, callback)

        except requests.RequestException as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è {archive_url}: {e}")
            self.failed_urls.append({
                'url': archive_url,
                'error': str(e),
                'error_type': 'RequestException',
                'depth': depth
            })

        except Exception as e:
            self.logger.error(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –¥–ª—è {archive_url}: {e}")
            self.failed_urls.append({
                'url': archive_url,
                'error': str(e),
                'error_type': 'UnexpectedException',
                'depth': depth
            })

    def _process_successful_page(self, archive_url: str, response, target_domain: str, depth: int):
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É."""

        # –ü–∞—Ä—Å–∏–º —Ç–æ–ª—å–∫–æ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        soup = BeautifulSoup(response.text, 'html.parser')

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        timestamp, original_url = ArchiveUrlHelper.extract_timestamp_and_original(archive_url)

        # –°–æ–±–∏—Ä–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        page_info = {
            'archive_url': archive_url,
            'original_url': original_url,
            'timestamp': timestamp,
            'title': self._extract_title_safe(soup),
            'content_length': len(response.text),
            'content_length_mb': round(len(response.text) / (1024 * 1024), 3),
            'depth': depth,
            'links_found': len(soup.find_all('a', href=True)),
            'images_found': len(soup.find_all('img')),
            'processed_at': time.time(),
            'response_time_ms': round(response.elapsed.total_seconds() * 1000, 2)
        }

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self.stats['pages_processed'] += 1
        self.stats['total_size_mb'] += page_info['content_length_mb']
        self.stats['avg_page_size'] = (
                (self.stats['avg_page_size'] * (self.stats['pages_processed'] - 1) +
                 len(response.text)) / self.stats['pages_processed']
        )

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
        file_path = self.storage_manager.save_page_content(
            archive_url=archive_url,
            content=response.text,
            metadata=page_info
        )

        page_info['saved_to'] = str(file_path)
        self.found_pages.append(page_info)

        self.logger.info(
            f"‚úÖ [{len(self.found_pages)}/{self.max_pages}] "
            f"{page_info['title'][:50]}... "
            f"({page_info['content_length_mb']} MB)"
        )

    def _extract_title_safe(self, soup: BeautifulSoup) -> str:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ –∏–∑–≤–ª–µ—á—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""
        try:
            if soup.title and soup.title.string:
                return soup.title.string.strip()

            h1_tag = soup.find('h1')
            if h1_tag and h1_tag.get_text():
                return h1_tag.get_text().strip()

            return "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
        except:
            return "–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞"

    def _extract_internal_links_optimized(
            self,
            html_content: str,
            base_archive_url: str,
            target_domain: str
    ) -> List[str]:
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫."""

        try:
            # –ü–∞—Ä—Å–∏–º —Ç–æ–ª—å–∫–æ –¥–ª—è —Å—Å—ã–ª–æ–∫
            soup = BeautifulSoup(html_content, 'html.parser')
            timestamp, _ = ArchiveUrlHelper.extract_timestamp_and_original(base_archive_url)

            internal_links = []
            links = soup.find_all('a', href=True)

            for link in links:
                href = link['href'].strip()
                if not href or href.startswith('#'):
                    continue

                # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∏—Å–∫–ª—é—á–∞–µ–º—ã–µ –ø—É—Ç–∏
                if any(exclude in href.lower() for exclude in self.exclude_paths):
                    continue

                archive_link = None

                # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è —Å—Å—ã–ª–∫–∞
                if href.startswith('/'):
                    archive_link = ArchiveUrlHelper.convert_relative_to_archive(href, base_archive_url)

                # –ê–±—Å–æ–ª—é—Ç–Ω–∞—è —Å—Å—ã–ª–∫–∞ —Ç–æ–≥–æ –∂–µ –¥–æ–º–µ–Ω–∞
                elif target_domain in href.lower():
                    if ArchiveUrlHelper.is_archive_url(href):
                        archive_link = href
                    else:
                        archive_link = ArchiveUrlHelper.build_archive_url(timestamp, href)

                # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞–ª–∏–¥–Ω—É—é —Å—Å—ã–ª–∫—É
                if (archive_link and
                        archive_link not in self.visited_urls and
                        ArchiveUrlHelper.is_same_domain(archive_link, base_archive_url)):

                    internal_links.append(archive_link)

            # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
            unique_links = list(set(internal_links))

            # –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É–µ–º –≤–∞–∂–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã
            priority_links = []
            regular_links = []

            for link in unique_links:
                is_priority = any(priority in link.lower() for priority in self.priority_paths)
                if is_priority:
                    priority_links.append(link)
                else:
                    regular_links.append(link)

            return priority_links + regular_links

        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Å—ã–ª–æ–∫: {e}")
            return []

    def _load_previous_state(self, archive_url: str):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–µ–¥—ã–¥—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è resume."""
        try:
            snapshot_path = self.storage_manager.get_snapshot_path(archive_url)
            state_file = snapshot_path / 'crawler_state.json'

            if state_file.exists():
                with open(state_file, 'r', encoding='utf-8') as f:
                    state = json.load(f)

                self.visited_urls = set(state.get('visited_urls', []))
                self.found_pages = state.get('found_pages', [])
                self.failed_urls = state.get('failed_urls', [])

                self.logger.info(f"üîÑ –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ —Å–æ—Å—Ç–æ—è–Ω–∏–µ: {len(self.visited_urls)} –ø–æ—Å–µ—â–µ–Ω–Ω—ã—Ö URL")

        except Exception as e:
            self.logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–µ–¥—ã–¥—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ: {e}")

    def _save_crawler_state(self, archive_url: str, summary: Dict[str, Any]):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∫—Ä–∞—É–ª–µ—Ä–∞."""
        try:
            snapshot_path = self.storage_manager.get_snapshot_path(archive_url)
            state_file = snapshot_path / 'crawler_state.json'

            state = {
                'visited_urls': list(self.visited_urls),
                'found_pages': self.found_pages,
                'failed_urls': self.failed_urls,
                'summary': summary,
                'saved_at': datetime.now().isoformat()
            }

            with open(state_file, 'w', encoding='utf-8') as f:
                json.dump(state, f, indent=2, ensure_ascii=False)

        except Exception as e:
            self.logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ: {e}")