"""–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∑–∞–≥—Ä—É–∑—á–∏–∫ —Å–Ω–∞–ø—à–æ—Ç–æ–≤ –∏–∑ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞ URL."""

import asyncio
import aiohttp
import json
import time
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
from bs4 import BeautifulSoup

from .storage_manager import StorageManager
from ..utils.rate_limiter import RateLimiter


class SnapshotDownloader:
    """–ó–∞–≥—Ä—É–∑—á–∏–∫ –¥–ª—è –º–∞—Å—Å–æ–≤–æ–≥–æ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è —Å–Ω–∞–ø—à–æ—Ç–æ–≤ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –¥–∞—Ç—É."""

    def __init__(
            self,
            storage_manager: StorageManager,
            rate_limiter: RateLimiter,
            max_concurrent: int = 3,
            resume_mode: bool = True
    ):
        self.storage_manager = storage_manager
        self.rate_limiter = rate_limiter
        self.max_concurrent = max_concurrent
        self.resume_mode = resume_mode

        self.logger = logging.getLogger(__name__)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'start_time': None,
            'successful': 0,
            'failed': 0,
            'skipped': 0,
            'total_size_mb': 0
        }

        # –°–ø–∏—Å–æ–∫ –Ω–µ—É–¥–∞—á–Ω—ã—Ö –∑–∞–≥—Ä—É–∑–æ–∫ –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫
        self.failed_downloads = []

    async def download_snapshot_batch(
            self,
            domain: str,
            date: str,
            snapshots: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        –°–∫–∞—á–∞—Ç—å –≤–µ—Å—å batch —Å–Ω–∞–ø—à–æ—Ç–æ–≤ –¥–ª—è –¥–æ–º–µ–Ω–∞ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –¥–∞—Ç—É.

        Args:
            domain: –î–æ–º–µ–Ω–Ω–æ–µ –∏–º—è
            date: –î–∞—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ YYYYMMDD
            snapshots: –°–ø–∏—Å–æ–∫ —Å–Ω–∞–ø—à–æ—Ç–æ–≤ –∏–∑ find_snapshots_for_date.sh

        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
        """

        self.logger.info(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ {len(snapshots)} —Å–Ω–∞–ø—à–æ—Ç–æ–≤ –¥–ª—è {domain} –Ω–∞ {date}")
        self.stats['start_time'] = time.time()

        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Å–Ω–∞–ø—à–æ—Ç–∞
        snapshot_dir = self.storage_manager.base_path / "snapshots" / domain / date
        snapshot_dir.mkdir(parents=True, exist_ok=True)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º resume —Ä–µ–∂–∏–º
        if self.resume_mode:
            snapshots = self._filter_existing_pages(snapshots, snapshot_dir)
            self.logger.info(f"üìÇ Resume —Ä–µ–∂–∏–º: –æ—Å—Ç–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å {len(snapshots)} —Å—Ç—Ä–∞–Ω–∏—Ü")

        if not snapshots:
            self.logger.info("‚úÖ –í—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —É–∂–µ —Å–∫–∞—á–∞–Ω—ã")
            return self._build_result_summary(domain, date, 0)

        # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ
        connector = aiohttp.TCPConnector(
            limit=self.max_concurrent,
            limit_per_host=self.max_concurrent,
            keepalive_timeout=60
        )

        timeout = aiohttp.ClientTimeout(total=60, connect=30)

        async with aiohttp.ClientSession(
                connector=connector,
                timeout=timeout,
                headers={'User-Agent': 'Mozilla/5.0 (compatible; WaybackAnalyzer/1.0)'}
        ) as session:

            # –°–æ–∑–¥–∞–µ–º —Å–µ–º–∞—Ñ–æ—Ä –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏
            semaphore = asyncio.Semaphore(self.max_concurrent)

            # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –≤—Å–µ—Ö —Å–Ω–∞–ø—à–æ—Ç–æ–≤
            tasks = []
            for i, snapshot in enumerate(snapshots):
                task = self._download_single_snapshot(
                    session, semaphore, snapshot, domain, date, i + 1, len(snapshots)
                )
                tasks.append(task)

            # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ —Å –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            self.logger.info(f"‚¨áÔ∏è  –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ ({self.max_concurrent} –ø–æ—Ç–æ–∫–æ–≤)")

            results = await asyncio.gather(*tasks, return_exceptions=True)

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            for result in results:
                if isinstance(result, Exception):
                    self.stats['failed'] += 1
                    self.logger.error(f"‚ùå –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏: {result}")
                elif result:
                    self.stats['successful'] += 1
                else:
                    self.stats['failed'] += 1

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–∞–Ω–∏—Ñ–µ—Å—Ç —Å–Ω–∞–ø—à–æ—Ç–∞
        await self._save_snapshot_manifest(domain, date, snapshots)

        # –ü–æ–≤—Ç–æ—Ä–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏ –¥–ª—è –Ω–µ—É–¥–∞—á–Ω—ã—Ö –∑–∞–≥—Ä—É–∑–æ–∫
        if self.failed_downloads:
            self.logger.info(f"üîÑ –ü–æ–≤—Ç–æ—Ä–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏ –¥–ª—è {len(self.failed_downloads)} –Ω–µ—É–¥–∞—á–Ω—ã—Ö –∑–∞–≥—Ä—É–∑–æ–∫")
            await self._retry_failed_downloads(domain, date)

        return self._build_result_summary(domain, date, len(snapshots))

    async def _download_single_snapshot(
            self,
            session: aiohttp.ClientSession,
            semaphore: asyncio.Semaphore,
            snapshot: Dict[str, Any],
            domain: str,
            date: str,
            current: int,
            total: int
    ) -> bool:
        """–°–∫–∞—á–∞—Ç—å –æ–¥–∏–Ω —Å–Ω–∞–ø—à–æ—Ç."""

        async with semaphore:
            # Rate limiting
            await asyncio.sleep(1.0 / (1.0 / self.rate_limiter.delay))

            archive_url = snapshot['archive_url']
            original_url = snapshot['original_url']

            try:
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                if current % 10 == 0 or current == 1:
                    self.logger.info(f"üìÑ [{current}/{total}] –°–∫–∞—á–∏–≤–∞—é: {original_url}")

                async with session.get(archive_url) as response:
                    if response.status == 200:
                        content = await response.text()

                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                        metadata = self._extract_page_metadata(content, snapshot)

                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
                        await self._save_page_to_snapshot_dir(
                            domain, date, original_url, content, metadata
                        )

                        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                        content_size_mb = len(content.encode('utf-8')) / (1024 * 1024)
                        self.stats['total_size_mb'] += content_size_mb

                        return True

                    else:
                        self.logger.warning(f"‚ö†Ô∏è  HTTP {response.status} –¥–ª—è {archive_url}")
                        self.failed_downloads.append(snapshot)
                        return False

            except asyncio.TimeoutError:
                self.logger.warning(f"‚è±Ô∏è  –¢–∞–π–º–∞—É—Ç –¥–ª—è {archive_url}")
                self.failed_downloads.append(snapshot)
                return False

            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ {archive_url}: {e}")
                self.failed_downloads.append(snapshot)
                return False

    def _extract_page_metadata(self, content: str, snapshot: Dict[str, Any]) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""

        try:
            soup = BeautifulSoup(content, 'html.parser')
            title = soup.title.string.strip() if soup.title and soup.title.string else "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
        except:
            title = "–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞"

        return {
            'archive_url': snapshot['archive_url'],
            'original_url': snapshot['original_url'],
            'timestamp': snapshot['timestamp'],
            'title': title,
            'content_length': len(content),
            'content_length_mb': round(len(content.encode('utf-8')) / (1024 * 1024), 3),
            'statuscode': snapshot.get('statuscode', '200'),
            'size': snapshot.get('size', 0),
            'days_diff': snapshot.get('days_diff', 0),
            'downloaded_at': datetime.now().isoformat(),
            'extracted_links': len(soup.find_all('a', href=True)) if 'soup' in locals() else 0,
            'extracted_images': len(soup.find_all('img')) if 'soup' in locals() else 0
        }

    async def _save_page_to_snapshot_dir(
            self,
            domain: str,
            date: str,
            original_url: str,
            content: str,
            metadata: Dict[str, Any]
    ):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —Å–Ω–∞–ø—à–æ—Ç–∞."""

        # –°–æ–∑–¥–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
        safe_filename = self._url_to_safe_filename(original_url)

        snapshot_dir = self.storage_manager.base_path / "snapshots" / domain / date

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML
        html_path = snapshot_dir / f"{safe_filename}.html"
        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(content)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        meta_path = snapshot_dir / f"{safe_filename}.html.meta.json"
        with open(meta_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)

    def _url_to_safe_filename(self, url: str) -> str:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å URL –≤ –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞."""
        import hashlib

        # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ—Ç–æ–∫–æ–ª
        if '://' in url:
            url = url.split('://', 1)[1]

        # –ó–∞–º–µ–Ω—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
        safe_name = url.replace('/', '_').replace('?', '_').replace('&', '_').replace('=', '_')
        safe_name = safe_name.replace(':', '_').replace('#', '_').replace('%', '_')

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
        if len(safe_name) > 150:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º hash –¥–ª—è –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã—Ö URL
            url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
            safe_name = safe_name[:140] + f"_{url_hash}"

        return safe_name

    def _filter_existing_pages(self, snapshots: List[Dict], snapshot_dir: Path) -> List[Dict]:
        """–§–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å —É–∂–µ —Å–∫–∞—á–∞–Ω–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è resume —Ä–µ–∂–∏–º–∞."""

        filtered = []
        for snapshot in snapshots:
            safe_filename = self._url_to_safe_filename(snapshot['original_url'])
            html_path = snapshot_dir / f"{safe_filename}.html"

            if not html_path.exists():
                filtered.append(snapshot)
            else:
                self.stats['skipped'] += 1

        return filtered

    async def _save_snapshot_manifest(
            self,
            domain: str,
            date: str,
            original_snapshots: List[Dict[str, Any]]
    ):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–∞–Ω–∏—Ñ–µ—Å—Ç —Å–Ω–∞–ø—à–æ—Ç–∞."""

        snapshot_dir = self.storage_manager.base_path / "snapshots" / domain / date
        manifest_path = snapshot_dir / "snapshot_manifest.json"

        manifest = {
            'domain': domain,
            'target_date': date,
            'created_at': datetime.now().isoformat(),
            'total_snapshots': len(original_snapshots),
            'successful_downloads': self.stats['successful'],
            'failed_downloads': self.stats['failed'],
            'skipped_existing': self.stats['skipped'],
            'total_size_mb': round(self.stats['total_size_mb'], 2),
            'duration_seconds': round(time.time() - self.stats['start_time'], 2),
            'snapshots_metadata': original_snapshots[:10]  # –ü–µ—Ä–≤—ã–µ 10 –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
        }

        with open(manifest_path, 'w', encoding='utf-8') as f:
            json.dump(manifest, f, indent=2, ensure_ascii=False)

    async def _retry_failed_downloads(self, domain: str, date: str):
        """–ü–æ–≤—Ç–æ—Ä–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏ –¥–ª—è –Ω–µ—É–¥–∞—á–Ω—ã—Ö –∑–∞–≥—Ä—É–∑–æ–∫."""

        if not self.failed_downloads:
            return

        retry_snapshots = self.failed_downloads.copy()
        self.failed_downloads.clear()

        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫–∏ –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫
        original_delay = self.rate_limiter.delay
        self.rate_limiter.delay *= 2  # –£–¥–≤–∞–∏–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É

        try:
            result = await self.download_snapshot_batch(domain, date, retry_snapshots)
            self.logger.info(f"üîÑ –ü–æ–≤—Ç–æ—Ä–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏: {result['successful']} —É—Å–ø–µ—à–Ω–æ")
        finally:
            self.rate_limiter.delay = original_delay

    def _build_result_summary(self, domain: str, date: str, total_attempted: int) -> Dict[str, Any]:
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –∏—Ç–æ–≥–æ–≤—É—é —Å–≤–æ–¥–∫—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤."""

        duration = time.time() - self.stats['start_time']

        return {
            'domain': domain,
            'date': date,
            'total_attempted': total_attempted,
            'successful': self.stats['successful'],
            'failed': self.stats['failed'],
            'skipped': self.stats['skipped'],
            'duration_seconds': round(duration, 2),
            'duration_minutes': round(duration / 60, 2),
            'total_size_mb': round(self.stats['total_size_mb'], 2),
            'pages_per_minute': round(self.stats['successful'] / (duration / 60), 1) if duration > 0 else 0,
            'average_page_size_kb': round((self.stats['total_size_mb'] * 1024) / max(self.stats['successful'], 1), 1)
        }