"""ÐÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ñ‹Ð¹ Ð·Ð°Ð³Ñ€ÑƒÐ·Ñ‡Ð¸Ðº ÑÐ½Ð°Ð¿ÑˆÐ¾Ñ‚Ð¾Ð² Ð¸Ð· Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²Ð»ÐµÐ½Ð½Ð¾Ð³Ð¾ ÑÐ¿Ð¸ÑÐºÐ° URL."""

import asyncio
import aiohttp
import json
import time
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
from bs4 import BeautifulSoup

from .storage_manager import StorageManager
from ..utils.rate_limiter import RateLimiter


class SnapshotDownloader:
    """Ð—Ð°Ð³Ñ€ÑƒÐ·Ñ‡Ð¸Ðº Ð´Ð»Ñ Ð¼Ð°ÑÑÐ¾Ð²Ð¾Ð³Ð¾ ÑÐºÐ°Ñ‡Ð¸Ð²Ð°Ð½Ð¸Ñ ÑÐ½Ð°Ð¿ÑˆÐ¾Ñ‚Ð¾Ð² Ð½Ð° ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½ÑƒÑŽ Ð´Ð°Ñ‚Ñƒ."""

    def __init__(
            self,
            storage_manager: StorageManager,
            rate_limiter: RateLimiter,
            max_concurrent: int = 3,
            resume_mode: bool = True
    ):
        self.storage_manager = storage_manager
        self.rate_limiter = rate_limiter
        self.max_concurrent = max_concurrent
        self.resume_mode = resume_mode

        self.logger = logging.getLogger(__name__)

        # Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°
        self.stats = {
            'start_time': None,
            'successful': 0,
            'failed': 0,
            'skipped': 0,
            'total_size_mb': 0
        }

        # Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð½ÐµÑƒÐ´Ð°Ñ‡Ð½Ñ‹Ñ… Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¾Ðº Ð´Ð»Ñ Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€Ð½Ñ‹Ñ… Ð¿Ð¾Ð¿Ñ‹Ñ‚Ð¾Ðº
        self.failed_downloads = []

    async def download_snapshot_batch(
            self,
            domain: str,
            date: str,
            snapshots: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Ð¡ÐºÐ°Ñ‡Ð°Ñ‚ÑŒ Ð²ÐµÑÑŒ batch ÑÐ½Ð°Ð¿ÑˆÐ¾Ñ‚Ð¾Ð² Ð´Ð»Ñ Ð´Ð¾Ð¼ÐµÐ½Ð° Ð½Ð° ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½ÑƒÑŽ Ð´Ð°Ñ‚Ñƒ.

        Args:
            domain: Ð”Ð¾Ð¼ÐµÐ½Ð½Ð¾Ðµ Ð¸Ð¼Ñ
            date: Ð”Ð°Ñ‚Ð° Ð² Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ðµ YYYYMMDD
            snapshots: Ð¡Ð¿Ð¸ÑÐ¾Ðº ÑÐ½Ð°Ð¿ÑˆÐ¾Ñ‚Ð¾Ð² Ð¸Ð· find_snapshots_for_date.sh

        Returns:
            Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° ÑÐºÐ°Ñ‡Ð¸Ð²Ð°Ð½Ð¸Ñ
        """

        self.logger.info(f"ðŸš€ ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ ÑÐºÐ°Ñ‡Ð¸Ð²Ð°Ð½Ð¸Ðµ {len(snapshots)} ÑÐ½Ð°Ð¿ÑˆÐ¾Ñ‚Ð¾Ð² Ð´Ð»Ñ {domain} Ð½Ð° {date}")
        self.stats['start_time'] = time.time()

        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸ÑŽ Ð´Ð»Ñ ÑÐ½Ð°Ð¿ÑˆÐ¾Ñ‚Ð°
        snapshot_dir = self.storage_manager.base_path / "snapshots" / domain / date
        snapshot_dir.mkdir(parents=True, exist_ok=True)

        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ resume Ñ€ÐµÐ¶Ð¸Ð¼
        if self.resume_mode:
            snapshots = self._filter_existing_pages(snapshots, snapshot_dir)
            self.logger.info(f"ðŸ“‚ Resume Ñ€ÐµÐ¶Ð¸Ð¼: Ð¾ÑÑ‚Ð°Ð»Ð¾ÑÑŒ ÑÐºÐ°Ñ‡Ð°Ñ‚ÑŒ {len(snapshots)} ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†")

        if not snapshots:
            self.logger.info("âœ… Ð’ÑÐµ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹ ÑƒÐ¶Ðµ ÑÐºÐ°Ñ‡Ð°Ð½Ñ‹")
            return self._build_result_summary(domain, date, 0)

        # ÐÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ð¾Ðµ ÑÐºÐ°Ñ‡Ð¸Ð²Ð°Ð½Ð¸Ðµ
        connector = aiohttp.TCPConnector(
            limit=self.max_concurrent,
            limit_per_host=self.max_concurrent,
            keepalive_timeout=60
        )

        timeout = aiohttp.ClientTimeout(total=60, connect=30)

        async with aiohttp.ClientSession(
                connector=connector,
                timeout=timeout,
                headers={'User-Agent': 'Mozilla/5.0 (compatible; WaybackAnalyzer/1.0)'}
        ) as session:

            # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ ÑÐµÐ¼Ð°Ñ„Ð¾Ñ€ Ð´Ð»Ñ Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ñ ÐºÐ¾Ð½ÐºÑƒÑ€ÐµÐ½Ñ‚Ð½Ð¾ÑÑ‚Ð¸
            semaphore = asyncio.Semaphore(self.max_concurrent)

            # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð·Ð°Ð´Ð°Ñ‡Ð¸ Ð´Ð»Ñ Ð²ÑÐµÑ… ÑÐ½Ð°Ð¿ÑˆÐ¾Ñ‚Ð¾Ð²
            tasks = []
            for i, snapshot in enumerate(snapshots):
                task = self._download_single_snapshot(
                    session, semaphore, snapshot, domain, date, i + 1, len(snapshots)
                )
                tasks.append(task)

            # Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÐ¼ Ð²ÑÐµ Ð·Ð°Ð´Ð°Ñ‡Ð¸ Ñ Ð¾Ñ‚Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸ÐµÐ¼ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑÐ°
            self.logger.info(f"â¬‡ï¸  ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð¾Ðµ ÑÐºÐ°Ñ‡Ð¸Ð²Ð°Ð½Ð¸Ðµ ({self.max_concurrent} Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð²)")

            results = await asyncio.gather(*tasks, return_exceptions=True)

            # ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹
            for result in results:
                if isinstance(result, Exception):
                    self.stats['failed'] += 1
                    self.logger.error(f"âŒ Ð˜ÑÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Ð¿Ñ€Ð¸ ÑÐºÐ°Ñ‡Ð¸Ð²Ð°Ð½Ð¸Ð¸: {result}")
                elif result:
                    self.stats['successful'] += 1
                else:
                    self.stats['failed'] += 1

        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¼Ð°Ð½Ð¸Ñ„ÐµÑÑ‚ ÑÐ½Ð°Ð¿ÑˆÐ¾Ñ‚Ð°
        await self._save_snapshot_manifest(domain, date, snapshots)

        # ÐŸÐ¾Ð²Ñ‚Ð¾Ñ€Ð½Ñ‹Ðµ Ð¿Ð¾Ð¿Ñ‹Ñ‚ÐºÐ¸ Ð´Ð»Ñ Ð½ÐµÑƒÐ´Ð°Ñ‡Ð½Ñ‹Ñ… Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¾Ðº
        if self.failed_downloads:
            self.logger.info(f"ðŸ”„ ÐŸÐ¾Ð²Ñ‚Ð¾Ñ€Ð½Ñ‹Ðµ Ð¿Ð¾Ð¿Ñ‹Ñ‚ÐºÐ¸ Ð´Ð»Ñ {len(self.failed_downloads)} Ð½ÐµÑƒÐ´Ð°Ñ‡Ð½Ñ‹Ñ… Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¾Ðº")
            await self._retry_failed_downloads(domain, date)

        return self._build_result_summary(domain, date, len(snapshots))

    async def _download_single_snapshot(
            self,
            session: aiohttp.ClientSession,
            semaphore: asyncio.Semaphore,
            snapshot: Dict[str, Any],
            domain: str,
            date: str,
            current: int,
            total: int
    ) -> bool:
        """Ð¡ÐºÐ°Ñ‡Ð°Ñ‚ÑŒ Ð¾Ð´Ð¸Ð½ ÑÐ½Ð°Ð¿ÑˆÐ¾Ñ‚."""

        async with semaphore:
            # Rate limiting
            await asyncio.sleep(1.0 / (1.0 / self.rate_limiter.delay))

            archive_url = snapshot['archive_url']
            original_url = snapshot['original_url']

            try:
                # ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑ
                if current % 10 == 0 or current == 1:
                    self.logger.info(f"ðŸ“„ [{current}/{total}] Ð¡ÐºÐ°Ñ‡Ð¸Ð²Ð°ÑŽ: {original_url}")

                async with session.get(archive_url) as response:
                    if response.status == 200:
                        content = await response.text()

                        # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð±Ð°Ð·Ð¾Ð²Ñ‹Ðµ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ
                        metadata = self._extract_page_metadata(content, snapshot)

                        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñƒ
                        await self._save_page_to_snapshot_dir(
                            domain, date, original_url, content, metadata
                        )

                        # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ
                        content_size_mb = len(content.encode('utf-8')) / (1024 * 1024)
                        self.stats['total_size_mb'] += content_size_mb

                        return True

                    else:
                        self.logger.warning(f"âš ï¸  HTTP {response.status} Ð´Ð»Ñ {archive_url}")
                        self.failed_downloads.append(snapshot)
                        return False

            except asyncio.TimeoutError:
                self.logger.warning(f"â±ï¸  Ð¢Ð°Ð¹Ð¼Ð°ÑƒÑ‚ Ð´Ð»Ñ {archive_url}")
                self.failed_downloads.append(snapshot)
                return False

            except Exception as e:
                self.logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ ÑÐºÐ°Ñ‡Ð¸Ð²Ð°Ð½Ð¸Ð¸ {archive_url}: {e}")
                self.failed_downloads.append(snapshot)
                return False

    def _extract_page_metadata(self, content: str, snapshot: Dict[str, Any]) -> Dict[str, Any]:
        """Ð˜Ð·Ð²Ð»ÐµÑ‡ÑŒ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹."""

        try:
            soup = BeautifulSoup(content, 'html.parser')
            title = soup.title.string.strip() if soup.title and soup.title.string else "Ð‘ÐµÐ· Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ°"
        except:
            title = "ÐžÑˆÐ¸Ð±ÐºÐ° Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ°"

        return {
            'archive_url': snapshot['archive_url'],
            'original_url': snapshot['original_url'],
            'timestamp': snapshot['timestamp'],
            'title': title,
            'content_length': len(content),
            'content_length_mb': round(len(content.encode('utf-8')) / (1024 * 1024), 3),
            'statuscode': snapshot.get('statuscode', '200'),
            'size': snapshot.get('size', 0),
            'days_diff': snapshot.get('days_diff', 0),
            'downloaded_at': datetime.now().isoformat(),
            'extracted_links': len(soup.find_all('a', href=True)) if 'soup' in locals() else 0,
            'extracted_images': len(soup.find_all('img')) if 'soup' in locals() else 0
        }

    async def _save_page_to_snapshot_dir(
            self,
            domain: str,
            date: str,
            original_url: str,
            content: str,
            metadata: Dict[str, Any]
    ):
        """Ð¡Ð¾Ñ…Ñ€Ð°Ð½Ð¸Ñ‚ÑŒ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñƒ Ð² Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸ÑŽ ÑÐ½Ð°Ð¿ÑˆÐ¾Ñ‚Ð°."""

        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾Ðµ Ð¸Ð¼Ñ Ñ„Ð°Ð¹Ð»Ð°
        safe_filename = self._url_to_safe_filename(original_url)

        snapshot_dir = self.storage_manager.base_path / "snapshots" / domain / date

        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ HTML
        html_path = snapshot_dir / f"{safe_filename}.html"
        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(content)

        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ
        meta_path = snapshot_dir / f"{safe_filename}.html.meta.json"
        with open(meta_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)

    def _url_to_safe_filename(self, url: str) -> str:
        """ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ñ‚ÑŒ URL Ð² Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾Ðµ Ð¸Ð¼Ñ Ñ„Ð°Ð¹Ð»Ð°."""
        import hashlib

        # Ð£Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð¿Ñ€Ð¾Ñ‚Ð¾ÐºÐ¾Ð»
        if '://' in url:
            url = url.split('://', 1)[1]

        # Ð—Ð°Ð¼ÐµÐ½ÑÐµÐ¼ ÑÐ¿ÐµÑ†Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ðµ ÑÐ¸Ð¼Ð²Ð¾Ð»Ñ‹
        safe_name = url.replace('/', '_').replace('?', '_').replace('&', '_').replace('=', '_')
        safe_name = safe_name.replace(':', '_').replace('#', '_').replace('%', '_')

        # ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡Ð¸Ð²Ð°ÐµÐ¼ Ð´Ð»Ð¸Ð½Ñƒ
        if len(safe_name) > 150:
            # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ hash Ð´Ð»Ñ Ð¾Ñ‡ÐµÐ½ÑŒ Ð´Ð»Ð¸Ð½Ð½Ñ‹Ñ… URL
            url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
            safe_name = safe_name[:140] + f"_{url_hash}"

        return safe_name

    def _filter_existing_pages(self, snapshots: List[Dict], snapshot_dir: Path) -> List[Dict]:
        """Ð¤Ð¸Ð»ÑŒÑ‚Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ ÑƒÐ¶Ðµ ÑÐºÐ°Ñ‡Ð°Ð½Ð½Ñ‹Ðµ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹ Ð´Ð»Ñ resume Ñ€ÐµÐ¶Ð¸Ð¼Ð°."""

        filtered = []
        for snapshot in snapshots:
            safe_filename = self._url_to_safe_filename(snapshot['original_url'])
            html_path = snapshot_dir / f"{safe_filename}.html"

            if not html_path.exists():
                filtered.append(snapshot)
            else:
                self.stats['skipped'] += 1

        return filtered

    async def _save_snapshot_manifest(
            self,
            domain: str,
            date: str,
            original_snapshots: List[Dict[str, Any]]
    ):
        """Ð¡Ð¾Ñ…Ñ€Ð°Ð½Ð¸Ñ‚ÑŒ Ð¼Ð°Ð½Ð¸Ñ„ÐµÑÑ‚ ÑÐ½Ð°Ð¿ÑˆÐ¾Ñ‚Ð°."""

        snapshot_dir = self.storage_manager.base_path / "snapshots" / domain / date
        manifest_path = snapshot_dir / "snapshot_manifest.json"

        manifest = {
            'domain': domain,
            'target_date': date,
            'created_at': datetime.now().isoformat(),
            'total_snapshots': len(original_snapshots),
            'successful_downloads': self.stats['successful'],
            'failed_downloads': self.stats['failed'],
            'skipped_existing': self.stats['skipped'],
            'total_size_mb': round(self.stats['total_size_mb'], 2),
            'duration_seconds': round(time.time() - self.stats['start_time'], 2),
            'snapshots_metadata': original_snapshots[:10]  # ÐŸÐµÑ€Ð²Ñ‹Ðµ 10 Ð´Ð»Ñ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð°
        }

        with open(manifest_path, 'w', encoding='utf-8') as f:
            json.dump(manifest, f, indent=2, ensure_ascii=False)

    async def _retry_failed_downloads(self, domain: str, date: str):
        """ÐŸÐ¾Ð²Ñ‚Ð¾Ñ€Ð½Ñ‹Ðµ Ð¿Ð¾Ð¿Ñ‹Ñ‚ÐºÐ¸ Ð´Ð»Ñ Ð½ÐµÑƒÐ´Ð°Ñ‡Ð½Ñ‹Ñ… Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¾Ðº."""

        if not self.failed_downloads:
            return

        retry_snapshots = self.failed_downloads.copy()
        self.failed_downloads.clear()

        # Ð£Ð²ÐµÐ»Ð¸Ñ‡Ð¸Ð²Ð°ÐµÐ¼ Ð·Ð°Ð´ÐµÑ€Ð¶ÐºÐ¸ Ð´Ð»Ñ Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€Ð½Ñ‹Ñ… Ð¿Ð¾Ð¿Ñ‹Ñ‚Ð¾Ðº
        original_delay = self.rate_limiter.delay
        self.rate_limiter.delay *= 2  # Ð£Ð´Ð²Ð°Ð¸Ð²Ð°ÐµÐ¼ Ð·Ð°Ð´ÐµÑ€Ð¶ÐºÑƒ

        try:
            result = await self.download_snapshot_batch(domain, date, retry_snapshots)
            self.logger.info(f"ðŸ”„ ÐŸÐ¾Ð²Ñ‚Ð¾Ñ€Ð½Ñ‹Ðµ Ð¿Ð¾Ð¿Ñ‹Ñ‚ÐºÐ¸: {result['successful']} ÑƒÑÐ¿ÐµÑˆÐ½Ð¾")
        finally:
            self.rate_limiter.delay = original_delay

    def _build_result_summary(self, domain: str, date: str, total_attempted: int) -> Dict[str, Any]:
        """ÐŸÐ¾ÑÑ‚Ñ€Ð¾Ð¸Ñ‚ÑŒ Ð¸Ñ‚Ð¾Ð³Ð¾Ð²ÑƒÑŽ ÑÐ²Ð¾Ð´ÐºÑƒ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð²."""

        duration = time.time() - self.stats['start_time']

        return {
            'domain': domain,
            'date': date,
            'total_attempted': total_attempted,
            'successful': self.stats['successful'],
            'failed': self.stats['failed'],
            'skipped': self.stats['skipped'],
            'duration_seconds': round(duration, 2),
            'duration_minutes': round(duration / 60, 2),
            'total_size_mb': round(self.stats['total_size_mb'], 2),
            'pages_per_minute': round(self.stats['successful'] / (duration / 60), 1) if duration > 0 else 0,
            'average_page_size_kb': round((self.stats['total_size_mb'] * 1024) / max(self.stats['successful'], 1), 1)
        }